{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with TensorFlow API\n",
    "\n",
    "To get this running, go to models/research dir and run the command: \"protoc object_detection/protos/*.protos --python_out=.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiag\\AppData\\Local\\Continuum\\anaconda3\\envs\\dlnd\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Setting up the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_dir_path = './models/research'\n",
    "models_dir_path = './models/research/object_detection'\n",
    "sys.path.append(research_dir_path)\n",
    "sys.path.append(models_dir_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Importing the actual API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiag\\AppData\\Local\\Continuum\\anaconda3\\envs\\dlnd\\lib\\site-packages\\matplotlib\\__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import object_detection\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_utils"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Setting the model the model and the labels for the detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "model_ckpt = '%s/%s/frozen_inference_graph.pb' % (models_dir_path, model_name)\n",
    "labels = '%s/data/mscoco_label_map.pbtxt' % (models_dir_path)\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here I get the graph, the key point was the ParseFromString function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = tf.Graph()\n",
    "with main_graph.as_default():\n",
    "    api_graph = tf.GraphDef()\n",
    "    with tf.gfile.GFile(model_ckpt, 'rb') as file:\n",
    "        serialized = file.read()\n",
    "        api_graph.ParseFromString(serialized)\n",
    "        tf.import_graph_def(api_graph, name='')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using some function from the API to get the labels and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(labels)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes = num_classes)\n",
    "categories_indexes = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Object Detection on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 1920) to (1088, 1920) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames processed:1655    Rate:3.6700653474982303\n"
     ]
    }
   ],
   "source": [
    "with main_graph.as_default():\n",
    "    with tf.Session(graph=main_graph) as sess:\n",
    "        #The Tensors from the graph\n",
    "        image_tensor = main_graph.get_tensor_by_name('image_tensor:0')\n",
    "        \n",
    "        detection_boxes = main_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        \n",
    "        detection_scores = main_graph.get_tensor_by_name('detection_scores:0')\n",
    "        \n",
    "        detection_classes = main_graph.get_tensor_by_name('detection_classes:0')\n",
    "        \n",
    "        num_detections = main_graph.get_tensor_by_name('num_detections:0')\n",
    "        #The name of the video, in the same dir of the notebook\n",
    "        video = 'bad_day'\n",
    "        #The reader and the writer\n",
    "        video_reader = imageio.get_reader('%s.mp4' % (video))\n",
    "        video_writer = imageio.get_writer('%s_edited.mp4' % video, fps = 30)\n",
    "        \n",
    "        num_frames = 0\n",
    "        t0 = datetime.now()\n",
    "        \n",
    "        #We capture the exception so it can take the max of frames before an error\n",
    "        try:\n",
    "            #Loop in the frames of the video\n",
    "            for frame in video_reader:\n",
    "            \n",
    "                arr_image = frame\n",
    "            \n",
    "                num_frames += 1\n",
    "                #Add one more dimension so the frame will be [1, width, height, channel]\n",
    "                arr_image_expanded = np.expand_dims(arr_image, axis = 0)\n",
    "            \n",
    "                feed = {image_tensor:arr_image_expanded}\n",
    "                #Run the detection\n",
    "                (boxes, scores, classes, num) = sess.run([detection_boxes, detection_scores, detection_classes, num_detections], feed_dict = feed)\n",
    "                #Puts everything together on the Frame\n",
    "                vis_utils.visualize_boxes_and_labels_on_image_array(arr_image, np.squeeze(boxes), np.squeeze(classes).astype(np.uint8), np.squeeze(scores), categories_indexes, use_normalized_coordinates = True, line_thickness = 8)\n",
    "                #Write the frame on the video\n",
    "                video_writer.append_data(arr_image)\n",
    "        \n",
    "        except RuntimeError:\n",
    "            print('Erro')\n",
    "        \n",
    "        #Basic calculation of the FPS\n",
    "        fps = num_frames/(datetime.now() - t0).total_seconds()\n",
    "        \n",
    "        print(\"Frames processed:{}    Rate:{}\".format(num_frames, fps))\n",
    "        \n",
    "        video_writer.close()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
